Setting UP A Data Pipeline to test conversation generation and to generate QA pairs for fine tuning a model
  I am working on fine-tuning a small model that can run on a 2060 GPU until I get 96GB 5090 MSI
  I want to do this all free and that is my goal
  This is all experimental so use at your own risk

###PURPOSE
  -I want to use small models because I find big models monolithic. Sure you get better results but you can get just as good results with embedding strategies, LoRA and quants
  -Just posting my learning experience for others who may be looking to do something like this

##WHAT YOU NEED
  Data. You can use Huggingface or Kaggle public datasets or you can generate your own synthetically to fine-tune.
  I am using Meta Synthetic Data Kit: https://github.com/meta-llama/synthetic-data-kit

##Requirements for synthetic data kit
  -Miniconda/conda or pure python
  -Local vLLM like Ollama or LM Studio or Api key for cloud LLM
  -Developer mode enabled for YAML

##WHAT NEXT
  -After installing the synthetic data kit feed it some documents. The instructions are pretty straightforward






